{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "#### Mandatory\n",
    "**`torch`**: PyTorch library contains the necessary tools to build deep learning models. It is used to train the model and make predictions.<br />\n",
    "**`yolov5`**: YOLOv5's library contains tools required to build the model. We use utils in this case to load the model correctly.<br />\n",
    "**`IPython`**: IPython library contains the necessary tools to display images in the notebook. It is used to display the images.<br />\n",
    "**`pathlib`**: pathlib is a Python library for object-oriented path manipulations. It is used to get the path of the images.<br />\n",
    "**`numpy`**: numpy library is used to work with arrays. It is used to convert the images to numpy arrays.<br />\n",
    "**`sklearn`**: sklearn library is used to perform machine learning tasks. It is used to calculate the mean squared error and such.<br />\n",
    "\n",
    "#### Optional\n",
    "**`yaml`**: yaml library is used to read the yaml files that contain the configuration of the model.<br />\n",
    "**`matplotlib`**: matplotlib is a Python library for creating static, animated, and interactive visualizations. It is also used to display the images.<br />\n",
    "**`glob`**: glob library is used to retrieve files/pathnames matching a specified pattern.<br />\n",
    "**`io`**: io library is used to handle various types of I/O (input/output) operations.<br />\n",
    "**`os`**: os library is used to interact with the operating system. It is used to create directories.<br />\n",
    "**`cv2`**: cv2 library is used to read and write images. Allows you to perform image processing and computer vision tasks.<br />\n",
    "**`json`**: json library is used to work with JSON data. It is used to read the JSON file that contains the labels.<br />\n",
    "**`shutil`**: shutil library is used to perform high-level operations on files and collections of files. It is used to copy the images to the output directory.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mandatory libraries.\n",
    "import torch\n",
    "from yolov5 import utils\n",
    "from IPython import display\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import shutil\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written functions\n",
    "#### (WIP) Model configuration\n",
    "**`model.conf`**: Used to set a threshold on the confidence of the model. This helps to filter out the predictions that are not accurate enough.<br />\n",
    "**`model.iou`**: Used to set a threshold on the (non maximum suppression) intersection over union of the model. This helps to filter out the predictions that are not accurate enough.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../PINT/yolov5/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m../PINT/yolov5/\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mcustom\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      2\u001b[0m                        path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../PINT/yolov5/runs/train/exp/weights/best.pt\u001b[39;49m\u001b[39m'\u001b[39;49m, source\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlocal\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39m# custom model\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[39m.\u001b[39mconf \u001b[39m=\u001b[39m \u001b[39m0.6\u001b[39m\n\u001b[1;32m      5\u001b[0m model\u001b[39m.\u001b[39miou \u001b[39m=\u001b[39m \u001b[39m0.45\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/divers/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/divers/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/divers/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '../PINT/yolov5/'"
     ]
    }
   ],
   "source": [
    "model = torch.load('../PINT/yolov5/', 'custom', \n",
    "                       path='../PINT/yolov5/runs/train/exp/weights/best.pt', source='local')  # custom model\n",
    "\n",
    "model.conf = 0.6\n",
    "model.iou = 0.45"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constants\n",
    "**`PROJECT_NAME`**: Name of the project.<br />\n",
    "**`BASE_MODEL`**: Name of the base model.<br />\n",
    "**`TRAIN_BATCH`**: Batch size used for training. Training batch is the number of training samples that will be propagated through the network in one forward/backward pass.<br />\n",
    "**`TRAIN_EPOCHS`**: Number of epochs used for training. Epochs are defined as the number of times the model will see the entire dataset.<br />\n",
    "**`VAL_BATCH`**: Batch size used for validation. Validation batch is the number of validation samples that will be propagated through the network in one forward/backward pass.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"divers_ml\"\n",
    "BASE_MODEL = \"yolov5m.pt\"\n",
    "TRAIN_BATCH = 5\n",
    "TRAIN_EPOCHS = 10\n",
    "VAL_BATCH = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming\n",
    "Cell text below strictly used to rename data in order for it to rename any input into the correct format that the model can process.<br />\n",
    "This folder hierarchy could _in theory_ be changed, but it's best we don't touch it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case there is new data introduced from same data set, re-name:\n",
    "\n",
    "# def move_files_to_dir(dirname):\n",
    "#     # /{dirname}/images\n",
    "#     for dir in os.listdir(os.path.join(dirname, 'images')):\n",
    "#         for file in os.listdir(os.path.join(dirname, 'images', dir)):\n",
    "#             image_file_name = dir + '_' + file\n",
    "#             shutil.move(os.path.join(dirname, 'images', dir, file), os.path.join(dirname, 'images', image_file_name))\n",
    "#         os.rmdir(os.path.join(dirname, 'images', dir))\n",
    "\n",
    "\n",
    "# # Move files outside of train and val and test\n",
    "# move_files_to_dir('data/test')\n",
    "# move_files_to_dir('data/train')\n",
    "# move_files_to_dir('data/valid')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data paths\n",
    "**`train_path`**: Leads to the training data (images).<br />\n",
    "**`test_path`**: Leads to the testing data.<br />\n",
    "**`valid_path`**: Leads to the validation data.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../data/train/images\"\n",
    "test_path = \"../data/test/images\"\n",
    "valid_path = \"../data/valid/images\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAML-reading\n",
    "Opening a file in write mode using `open()` and assigning it to the variable `file`. Using `yaml.dump()` to serialise a Python dictionary into a YAML format and write it to the opened file. <br/>\n",
    "The dictionary contains several key-value pairs:<br/>\n",
    "- The _\"train\"_ key contains the path to the training data.<br/>\n",
    "- The _\"test\"_ key contains the path to the testing data.<br/>\n",
    "- The _\"val\"_ key contains the path to the validation data.<br/>\n",
    "- The _\"nc\"_ key contains the number of classes.<br/>\n",
    "- The _\"names\"_ key contains the names of the objects that should be found.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.yaml\", \"w\") as file:\n",
    "    yaml.dump({\n",
    "        \"train\": train_path,\n",
    "        \"test\": test_path,\n",
    "        \"val\": valid_path,\n",
    "        \"nc\": 1,\n",
    "        \"names\": [\"diver\"]\n",
    "    }, stream=file, default_flow_style=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning\n",
    "#### Delete old data\n",
    "See below - This is a function that clears previous results and before training the model.<br/> \n",
    "Placed in a separate cell in case you wish to keep the previous results.<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old results -- Training.\n",
    "wildcard = f\"{PROJECT_NAME}/feature_extraction*\"\n",
    "! rm -r $wildcard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "The whole line below is the line responsible for running the training of the model. Let's visit the functionality of it in order to grasp how it functions.<br/>\n",
    "- `!` at the beginning suggests that this line might be executed in a Jupyter Notebook or a similar environment where shell commands can be run.<br/>\n",
    "- `python yolov5/train.py` invokes the Python interpreter to execute the train.py script located in the yolov5 directory. This implies that the script is expected to be executed with the Python language. <br/>\n",
    "- `--batch $TRAIN_BATCH` is a command-line argument passed to the `train.py` script. It specifies the batch size for training. The value of `$TRAIN_BATCH` is a placeholder replaced with the actual value (defined before) before running the command.<br/>\n",
    "- `--epochs $TRAIN_EPOCHS` is another command-line argument specifying the number of epochs for training. Similar to the previous argument, `$TRAIN_EPOCHS` is replaced with a specific value previously defined.<br/>\n",
    "- `--data \"data.yaml\"` is an argument specifying the path to a YAML file that contains data configuration for the training process. The file named \"`data.yaml`\" is expected to be present in the current directory.<br/>\n",
    "- `--weights $BASE_MODEL` specifies the path to the base model or pre-trained weights to be used for training. The value of $BASE_MODEL is replaced with the actual path to the desired model weights.<br/>\n",
    "- `--project $PROJECT_NAME` is an argument specifying the name of the project. It is used to organize the training outputs or checkpoints under a specific project name.<br/>\n",
    "- `--name 'feature_extraction'` specifies a name for the current training run. In this case, the name is set as \"`feature_extraction`\".<br/>\n",
    "- `--cache` is an argument indicating that caching should be enabled during training. Caching can help improve training performance by reducing data loading time.<br/>\n",
    "- `--freeze 12` specifies the number of initial layers or stages to freeze during training. In this case, the first 12 layers will be frozen, and only the remaining layers will be fine-tuned (Don't worry about what \"frozen\" means here).<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5m.pt, cfg=, data=data.yaml, hyp=yolov5/data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=5, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=divers_ml, name=feature_extraction, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[12], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
      "YOLOv5 🚀 v7.0-174-g5eb7f7d Python-3.10.11 torch-2.0.1+cu117 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir divers_ml', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      5280  models.common.Conv                      [3, 48, 6, 2, 2]              \n",
      "  1                -1  1     41664  models.common.Conv                      [48, 96, 3, 2]                \n",
      "  2                -1  2     65280  models.common.C3                        [96, 96, 2]                   \n",
      "  3                -1  1    166272  models.common.Conv                      [96, 192, 3, 2]               \n",
      "  4                -1  4    444672  models.common.C3                        [192, 192, 4]                 \n",
      "  5                -1  1    664320  models.common.Conv                      [192, 384, 3, 2]              \n",
      "  6                -1  6   2512896  models.common.C3                        [384, 384, 6]                 \n",
      "  7                -1  1   2655744  models.common.Conv                      [384, 768, 3, 2]              \n",
      "  8                -1  2   4134912  models.common.C3                        [768, 768, 2]                 \n",
      "  9                -1  1   1476864  models.common.SPPF                      [768, 768, 5]                 \n",
      " 10                -1  1    295680  models.common.Conv                      [768, 384, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  2   1182720  models.common.C3                        [768, 384, 2, False]          \n",
      " 14                -1  1     74112  models.common.Conv                      [384, 192, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  2    296448  models.common.C3                        [384, 192, 2, False]          \n",
      " 18                -1  1    332160  models.common.Conv                      [192, 192, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  2   1035264  models.common.C3                        [384, 384, 2, False]          \n",
      " 21                -1  1   1327872  models.common.Conv                      [384, 384, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  2   4134912  models.common.C3                        [768, 768, 2, False]          \n",
      " 24      [17, 20, 23]  1     24246  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [192, 384, 768]]\n",
      "Model summary: 291 layers, 20871318 parameters, 20871318 gradients, 48.2 GFLOPs\n",
      "\n",
      "Transferred 475/481 items from yolov5m.pt\n",
      "freezing model.0.conv.weight\n",
      "freezing model.0.bn.weight\n",
      "freezing model.0.bn.bias\n",
      "freezing model.1.conv.weight\n",
      "freezing model.1.bn.weight\n",
      "freezing model.1.bn.bias\n",
      "freezing model.2.cv1.conv.weight\n",
      "freezing model.2.cv1.bn.weight\n",
      "freezing model.2.cv1.bn.bias\n",
      "freezing model.2.cv2.conv.weight\n",
      "freezing model.2.cv2.bn.weight\n",
      "freezing model.2.cv2.bn.bias\n",
      "freezing model.2.cv3.conv.weight\n",
      "freezing model.2.cv3.bn.weight\n",
      "freezing model.2.cv3.bn.bias\n",
      "freezing model.2.m.0.cv1.conv.weight\n",
      "freezing model.2.m.0.cv1.bn.weight\n",
      "freezing model.2.m.0.cv1.bn.bias\n",
      "freezing model.2.m.0.cv2.conv.weight\n",
      "freezing model.2.m.0.cv2.bn.weight\n",
      "freezing model.2.m.0.cv2.bn.bias\n",
      "freezing model.2.m.1.cv1.conv.weight\n",
      "freezing model.2.m.1.cv1.bn.weight\n",
      "freezing model.2.m.1.cv1.bn.bias\n",
      "freezing model.2.m.1.cv2.conv.weight\n",
      "freezing model.2.m.1.cv2.bn.weight\n",
      "freezing model.2.m.1.cv2.bn.bias\n",
      "freezing model.3.conv.weight\n",
      "freezing model.3.bn.weight\n",
      "freezing model.3.bn.bias\n",
      "freezing model.4.cv1.conv.weight\n",
      "freezing model.4.cv1.bn.weight\n",
      "freezing model.4.cv1.bn.bias\n",
      "freezing model.4.cv2.conv.weight\n",
      "freezing model.4.cv2.bn.weight\n",
      "freezing model.4.cv2.bn.bias\n",
      "freezing model.4.cv3.conv.weight\n",
      "freezing model.4.cv3.bn.weight\n",
      "freezing model.4.cv3.bn.bias\n",
      "freezing model.4.m.0.cv1.conv.weight\n",
      "freezing model.4.m.0.cv1.bn.weight\n",
      "freezing model.4.m.0.cv1.bn.bias\n",
      "freezing model.4.m.0.cv2.conv.weight\n",
      "freezing model.4.m.0.cv2.bn.weight\n",
      "freezing model.4.m.0.cv2.bn.bias\n",
      "freezing model.4.m.1.cv1.conv.weight\n",
      "freezing model.4.m.1.cv1.bn.weight\n",
      "freezing model.4.m.1.cv1.bn.bias\n",
      "freezing model.4.m.1.cv2.conv.weight\n",
      "freezing model.4.m.1.cv2.bn.weight\n",
      "freezing model.4.m.1.cv2.bn.bias\n",
      "freezing model.4.m.2.cv1.conv.weight\n",
      "freezing model.4.m.2.cv1.bn.weight\n",
      "freezing model.4.m.2.cv1.bn.bias\n",
      "freezing model.4.m.2.cv2.conv.weight\n",
      "freezing model.4.m.2.cv2.bn.weight\n",
      "freezing model.4.m.2.cv2.bn.bias\n",
      "freezing model.4.m.3.cv1.conv.weight\n",
      "freezing model.4.m.3.cv1.bn.weight\n",
      "freezing model.4.m.3.cv1.bn.bias\n",
      "freezing model.4.m.3.cv2.conv.weight\n",
      "freezing model.4.m.3.cv2.bn.weight\n",
      "freezing model.4.m.3.cv2.bn.bias\n",
      "freezing model.5.conv.weight\n",
      "freezing model.5.bn.weight\n",
      "freezing model.5.bn.bias\n",
      "freezing model.6.cv1.conv.weight\n",
      "freezing model.6.cv1.bn.weight\n",
      "freezing model.6.cv1.bn.bias\n",
      "freezing model.6.cv2.conv.weight\n",
      "freezing model.6.cv2.bn.weight\n",
      "freezing model.6.cv2.bn.bias\n",
      "freezing model.6.cv3.conv.weight\n",
      "freezing model.6.cv3.bn.weight\n",
      "freezing model.6.cv3.bn.bias\n",
      "freezing model.6.m.0.cv1.conv.weight\n",
      "freezing model.6.m.0.cv1.bn.weight\n",
      "freezing model.6.m.0.cv1.bn.bias\n",
      "freezing model.6.m.0.cv2.conv.weight\n",
      "freezing model.6.m.0.cv2.bn.weight\n",
      "freezing model.6.m.0.cv2.bn.bias\n",
      "freezing model.6.m.1.cv1.conv.weight\n",
      "freezing model.6.m.1.cv1.bn.weight\n",
      "freezing model.6.m.1.cv1.bn.bias\n",
      "freezing model.6.m.1.cv2.conv.weight\n",
      "freezing model.6.m.1.cv2.bn.weight\n",
      "freezing model.6.m.1.cv2.bn.bias\n",
      "freezing model.6.m.2.cv1.conv.weight\n",
      "freezing model.6.m.2.cv1.bn.weight\n",
      "freezing model.6.m.2.cv1.bn.bias\n",
      "freezing model.6.m.2.cv2.conv.weight\n",
      "freezing model.6.m.2.cv2.bn.weight\n",
      "freezing model.6.m.2.cv2.bn.bias\n",
      "freezing model.6.m.3.cv1.conv.weight\n",
      "freezing model.6.m.3.cv1.bn.weight\n",
      "freezing model.6.m.3.cv1.bn.bias\n",
      "freezing model.6.m.3.cv2.conv.weight\n",
      "freezing model.6.m.3.cv2.bn.weight\n",
      "freezing model.6.m.3.cv2.bn.bias\n",
      "freezing model.6.m.4.cv1.conv.weight\n",
      "freezing model.6.m.4.cv1.bn.weight\n",
      "freezing model.6.m.4.cv1.bn.bias\n",
      "freezing model.6.m.4.cv2.conv.weight\n",
      "freezing model.6.m.4.cv2.bn.weight\n",
      "freezing model.6.m.4.cv2.bn.bias\n",
      "freezing model.6.m.5.cv1.conv.weight\n",
      "freezing model.6.m.5.cv1.bn.weight\n",
      "freezing model.6.m.5.cv1.bn.bias\n",
      "freezing model.6.m.5.cv2.conv.weight\n",
      "freezing model.6.m.5.cv2.bn.weight\n",
      "freezing model.6.m.5.cv2.bn.bias\n",
      "freezing model.7.conv.weight\n",
      "freezing model.7.bn.weight\n",
      "freezing model.7.bn.bias\n",
      "freezing model.8.cv1.conv.weight\n",
      "freezing model.8.cv1.bn.weight\n",
      "freezing model.8.cv1.bn.bias\n",
      "freezing model.8.cv2.conv.weight\n",
      "freezing model.8.cv2.bn.weight\n",
      "freezing model.8.cv2.bn.bias\n",
      "freezing model.8.cv3.conv.weight\n",
      "freezing model.8.cv3.bn.weight\n",
      "freezing model.8.cv3.bn.bias\n",
      "freezing model.8.m.0.cv1.conv.weight\n",
      "freezing model.8.m.0.cv1.bn.weight\n",
      "freezing model.8.m.0.cv1.bn.bias\n",
      "freezing model.8.m.0.cv2.conv.weight\n",
      "freezing model.8.m.0.cv2.bn.weight\n",
      "freezing model.8.m.0.cv2.bn.bias\n",
      "freezing model.8.m.1.cv1.conv.weight\n",
      "freezing model.8.m.1.cv1.bn.weight\n",
      "freezing model.8.m.1.cv1.bn.bias\n",
      "freezing model.8.m.1.cv2.conv.weight\n",
      "freezing model.8.m.1.cv2.bn.weight\n",
      "freezing model.8.m.1.cv2.bn.bias\n",
      "freezing model.9.cv1.conv.weight\n",
      "freezing model.9.cv1.bn.weight\n",
      "freezing model.9.cv1.bn.bias\n",
      "freezing model.9.cv2.conv.weight\n",
      "freezing model.9.cv2.bn.weight\n",
      "freezing model.9.cv2.bn.bias\n",
      "freezing model.10.conv.weight\n",
      "freezing model.10.bn.weight\n",
      "freezing model.10.bn.bias\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 79 weight(decay=0.0), 82 weight(decay=0.0005078125), 82 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/saritak/Documents/PINT/data/train/labels.cache... 314 imag\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.2GB ram): 100%|██████████| 314/314 [00:00<00:00, 315.02\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/saritak/Documents/PINT/data/valid/labels.cache... 97 images,\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|██████████| 97/97 [00:00<00:00, 267.81it/s\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.33 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
      "Plotting labels to divers_ml/feature_extraction2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 5 dataloader workers\n",
      "Logging results to \u001b[1mdivers_ml/feature_extraction2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9         0G    0.09319    0.03882          0          3        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.469      0.558       0.49      0.216\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9         0G    0.06871    0.03617          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.402      0.883      0.571      0.204\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9         0G    0.06364    0.03395          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.493        0.6        0.6      0.258\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9         0G    0.06102    0.03023          0         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.607      0.875      0.762      0.313\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9         0G    0.05276    0.02801          0         23        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.741        0.9      0.836       0.38\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9         0G    0.04878    0.02588          0         17        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.796      0.913      0.918      0.475\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9         0G    0.04325     0.0248          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.871      0.908      0.904       0.45\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9         0G    0.04122    0.02611          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.954      0.925      0.972      0.552\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9         0G    0.03837    0.02434          0         15        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120          1      0.928      0.983      0.563\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9         0G     0.0355    0.02291          0          8        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120      0.971      0.933      0.977      0.522\n",
      "\n",
      "10 epochs completed in 0.455 hours.\n",
      "Optimizer stripped from divers_ml/feature_extraction2/weights/last.pt, 42.1MB\n",
      "Optimizer stripped from divers_ml/feature_extraction2/weights/best.pt, 42.1MB\n",
      "\n",
      "Validating divers_ml/feature_extraction2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         97        120          1      0.928      0.983      0.562\n",
      "Results saved to \u001b[1mdivers_ml/feature_extraction2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python yolov5/train.py --batch $TRAIN_BATCH --epochs $TRAIN_EPOCHS --data \"data.yaml\" --weights $BASE_MODEL --project $PROJECT_NAME --name 'feature_extraction' --cache --freeze 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old validation results\n",
    "See below - This is a function that clears previous results and before validating the model.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old results - Validation data (Sensible? Feasibly?)\n",
    "wildcard = f\"{PROJECT_NAME}/validation_on_test_data*\"\n",
    "! rm -r $wildcard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate the model\n",
    "This script below is responsible to run the validation of the model. Let's visit the functionality of it in order to grasp how it functions.<br/>\n",
    "- `!` at the beginning suggests that this line might be executed in a Jupyter Notebook or a similar environment where shell commands can be run.<br/>\n",
    "- `python yolov5/detect.py` invokes the Python interpreter to execute the detect.py script located in the yolov5 directory. This implies that the script is expected to be executed with the Python language. <br/>\n",
    "- `--weights $WEIGHTS_BEST` is an argument specifying the path to the weights of the model to be used for validation. The path is relative to the current directory.<br/>\n",
    "- `--batch $VAL_BATCH` is a command-line argument passed to the `detect.py` script. It specifies the batch size for validation.<br/>\n",
    "- `--data 'data.yaml'` is an argument specifying the path to a YAML file that contains data configuration for the validation process. The file named \"`data.yaml`\" is expected to be present in the current directory.<br/>\n",
    "- `--task test` is an argument specifying the task to be performed. In this case, the task is set as \"`test`\".<br/>\n",
    "- `--project $PROJECT_NAME` is an argument specifying the name of the project. It is used to organize the validation outputs or checkpoints under a specific project name.<br/>\n",
    "- `--name 'validation_on_test_data'` specifies a name for the current validation run. In this case, the name is set as \"`validation_on_test_data`\".<br/>\n",
    "- `--augment` is an argument indicating that data augmentation should be enabled during validation. Data augmentation can help improve validation performance by increasing the number of validation samples.<br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data.yaml, weights=['divers_ml/feature_extraction/weights/best.pt'], batch_size=5, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=test, device=, workers=8, single_cls=False, augment=True, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=divers_ml, name=validation_on_test_data, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 🚀 v7.0-174-g5eb7f7d Python-3.10.11 torch-2.0.1+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning /home/saritak/Documents/PINT/data/test/labels... 4 images, 34 bac\u001b[0m\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: /home/saritak/Documents/PINT/data/test/labels.cache\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         38         12      0.199      0.833      0.217     0.0904\n",
      "Speed: 1.2ms pre-process, 469.6ms inference, 1.1ms NMS per image at shape (5, 3, 640, 640)\n",
      "Results saved to \u001b[1mdivers_ml/validation_on_test_data3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS_BEST = f\"{PROJECT_NAME}/feature_extraction/weights/best.pt\"\n",
    "! python yolov5/val.py --weights $WEIGHTS_BEST --batch $VAL_BATCH --data 'data.yaml' --task test --project $PROJECT_NAME --name 'validation_on_test_data' --augment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete old test results\n",
    "See below - This is a function that clears previous results and placed before testing in case a user wishes to keep previous results.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete old results - Detection test.\n",
    "wildcard = f\"{PROJECT_NAME}/detect_test*\"\n",
    "! rm -r $wildcard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing\n",
    "Testing will be done on the test data provided (images, mainly). The script below runs the test, so do beware when running it. It shouldn't take too long with the current amount of images.<br/>\n",
    "- `--weights $WEIGHTS_BEST` is an argument specifying the path to the weights of the model to be used for testing. The path is relative to the current directory.<br/>\n",
    "- `--conf #` is an argument specifying the confidence threshold. It is used to filter out predictions that are not accurate enough.<br/>\n",
    "- `--source $test_path` is an argument specifying the path to the test data. The path is relative to the current directory.<br/>\n",
    "- `--name 'detect_test'` specifies a name for the current testing run. In this case, the name is set as \"`detect_test`\".<br/>\n",
    "- `--augment` is an argument indicating that data augmentation should be enabled during testing. Data augmentation can help improve testing performance by increasing the number of testing samples.<br/>\n",
    "- `--line=3` is an argument specifying the line thickness of the bounding boxes.<br/>\n",
    "- `--iou #` is an argument specifying the intersection over union threshold. It is used to filter out predictions that are not accurate enough.<br/>\n",
    "- `--save-txt` is an argument indicating that the predicted bounding boxes should be saved in a text file.<br/>\n",
    "- `--save-conf` is an argument indicating that the confidence of the predicted bounding boxes should be saved in the text file.<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['divers_ml/feature_extraction/weights/best.pt'], source=data/test/images, data=yolov5/data/coco128.yaml, imgsz=[640, 640], conf_thres=0.6, iou_thres=0.2, max_det=1000, device=, view_img=False, save_txt=True, save_conf=True, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=True, visualize=False, update=False, project=divers_ml, name=detect_test, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 🚀 v7.0-174-g5eb7f7d Python-3.10.11 torch-2.0.1+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 212 layers, 20852934 parameters, 0 gradients, 47.9 GFLOPs\n",
      "image 1/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 1.png: 480x640 2 divers, 382.0ms\n",
      "image 2/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 10.png: 448x640 2 divers, 467.9ms\n",
      "image 3/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 11.png: 480x640 1 diver, 423.6ms\n",
      "image 4/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 12.png: 384x640 1 diver, 426.4ms\n",
      "image 5/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 2.png: 384x640 1 diver, 374.5ms\n",
      "image 6/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 3.png: 448x640 (no detections), 418.4ms\n",
      "image 7/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 4.png: 480x640 1 diver, 457.2ms\n",
      "image 8/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 5.png: 480x640 2 divers, 469.6ms\n",
      "image 9/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 6.png: 480x640 2 divers, 471.4ms\n",
      "image 10/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 7.png: 640x576 1 diver, 569.2ms\n",
      "image 11/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 8.png: 640x448 (no detections), 403.1ms\n",
      "image 12/38 /home/saritak/Documents/PINT/data/test/images/Pasted image 9.png: 384x640 1 diver, 333.9ms\n",
      "image 13/38 /home/saritak/Documents/PINT/data/test/images/Pasted image.png: 384x640 1 diver, 381.9ms\n",
      "image 14/38 /home/saritak/Documents/PINT/data/test/images/barbados_scuba_010_B_2403.jpg: 384x640 3 divers, 379.5ms\n",
      "image 15/38 /home/saritak/Documents/PINT/data/test/images/barbados_scuba_010_B_2404.jpg: 384x640 3 divers, 388.5ms\n",
      "image 16/38 /home/saritak/Documents/PINT/data/test/images/barbados_scuba_010_B_2409.jpg: 384x640 2 divers, 373.8ms\n",
      "image 17/38 /home/saritak/Documents/PINT/data/test/images/barbados_scuba_010_B_2410.jpg: 384x640 2 divers, 348.5ms\n",
      "image 18/38 /home/saritak/Documents/PINT/data/test/images/sealion1.png: 256x640 (no detections), 298.3ms\n",
      "image 19/38 /home/saritak/Documents/PINT/data/test/images/sealion2.png: 416x640 2 divers, 421.0ms\n",
      "image 20/38 /home/saritak/Documents/PINT/data/test/images/sealion4.png: 384x640 2 divers, 383.1ms\n",
      "image 21/38 /home/saritak/Documents/PINT/data/test/images/testimg1.png: 640x640 1 diver, 580.0ms\n",
      "image 22/38 /home/saritak/Documents/PINT/data/test/images/testimg10.png: 480x640 1 diver, 445.9ms\n",
      "image 23/38 /home/saritak/Documents/PINT/data/test/images/testimg11.png: 448x640 (no detections), 444.2ms\n",
      "image 24/38 /home/saritak/Documents/PINT/data/test/images/testimg12.png: 480x640 (no detections), 459.7ms\n",
      "image 25/38 /home/saritak/Documents/PINT/data/test/images/testimg13.png: 352x640 1 diver, 367.7ms\n",
      "image 26/38 /home/saritak/Documents/PINT/data/test/images/testimg14.png: 448x640 4 divers, 389.2ms\n",
      "image 27/38 /home/saritak/Documents/PINT/data/test/images/testimg15.png: 480x640 1 diver, 467.8ms\n",
      "image 28/38 /home/saritak/Documents/PINT/data/test/images/testimg16.png: 384x640 1 diver, 343.8ms\n",
      "image 29/38 /home/saritak/Documents/PINT/data/test/images/testimg17.png: 448x640 1 diver, 425.1ms\n",
      "image 30/38 /home/saritak/Documents/PINT/data/test/images/testimg18.png: 480x640 (no detections), 478.8ms\n",
      "image 31/38 /home/saritak/Documents/PINT/data/test/images/testimg2.png: 384x640 1 diver, 384.6ms\n",
      "image 32/38 /home/saritak/Documents/PINT/data/test/images/testimg3.png: 480x640 5 divers, 474.7ms\n",
      "image 33/38 /home/saritak/Documents/PINT/data/test/images/testimg4.png: 448x640 1 diver, 440.9ms\n",
      "image 34/38 /home/saritak/Documents/PINT/data/test/images/testimg5.png: 640x640 1 diver, 604.3ms\n",
      "image 35/38 /home/saritak/Documents/PINT/data/test/images/testimg6.png: 448x640 2 divers, 442.1ms\n",
      "image 36/38 /home/saritak/Documents/PINT/data/test/images/testimg7.png: 384x640 3 divers, 388.3ms\n",
      "image 37/38 /home/saritak/Documents/PINT/data/test/images/testimg8.png: 512x640 3 divers, 458.2ms\n",
      "image 38/38 /home/saritak/Documents/PINT/data/test/images/testimg9.png: 384x640 2 divers, 339.2ms\n",
      "Speed: 0.5ms pre-process, 423.9ms inference, 0.5ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mdivers_ml/detect_test\u001b[0m\n",
      "32 labels saved to divers_ml/detect_test/labels\n"
     ]
    }
   ],
   "source": [
    "! python yolov5/detect.py --weights $WEIGHTS_BEST --conf 0.6 --source 'data/test/images' --project $PROJECT_NAME --name 'detect_test' --augment --line=3 --iou 0.2 --save-txt --save-conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Algorithm; Take only outside box or highest confidence (I don't know how).\n",
    "\n",
    "# TODO: Increase data set, improve training. (Optional - Not to be done now)\n",
    "\n",
    "# TODO: Return boolean if diver is detected.\n",
    "\n",
    "# TODO: Return image with bounding box. <- We can do this with the coordinates and confidence. (Done?)\n",
    "    # TODO: Return coordinates of diver. <- Done, returns a txt file with coordinates of the diver within the image processed (like, where he is in that image - pixels).\n",
    "    # TODO: Return confidence of diver. <- Done, confidence is the last floating point value on the .txt file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "divers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
